{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd436882-9f2a-438c-b10e-69600f8b3562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8395bab-8874-40d3-8218-46fe1765a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('prepared_data.csv')\n",
    "df = df[['cleaned_text', 'LabelMapped']]\n",
    "df['LabelMapped'] = df['LabelMapped'].map({-1: 0, 0: 1, 1: 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f49a4425-fc42-4c0d-89ce-de7e48fe1eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>LabelMapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arrived broken manufacturer defect two of the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the cabinet dot were all detached from backing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i received my first order of this product and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this product is a piece of shit do not buy doe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>went through in one day doesnt fit correct and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255077</th>\n",
       "      <td>racaltosk ok good to know punting at metlife i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255078</th>\n",
       "      <td>everyone who sat around me at metlife was so a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255079</th>\n",
       "      <td>what giants or niners fans would wanna go to t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255080</th>\n",
       "      <td>anybody want a ticket for tomorrow colombia vs...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255081</th>\n",
       "      <td>mendez told me hed drive me to metlife on sund...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255082 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             cleaned_text  LabelMapped\n",
       "0       arrived broken manufacturer defect two of the ...            0\n",
       "1       the cabinet dot were all detached from backing...            0\n",
       "2       i received my first order of this product and ...            0\n",
       "3       this product is a piece of shit do not buy doe...            0\n",
       "4       went through in one day doesnt fit correct and...            0\n",
       "...                                                   ...          ...\n",
       "255077  racaltosk ok good to know punting at metlife i...            2\n",
       "255078  everyone who sat around me at metlife was so a...            1\n",
       "255079  what giants or niners fans would wanna go to t...            1\n",
       "255080  anybody want a ticket for tomorrow colombia vs...            2\n",
       "255081  mendez told me hed drive me to metlife on sund...            1\n",
       "\n",
       "[255082 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bb02da0-c6b5-43ee-945f-aeceeb3ec089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    df['cleaned_text'], df['LabelMapped'], test_size=0.3, random_state=42\n",
    ")\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21021d92-1d34-4e88-b76a-55cdb519e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_texts(texts, max_length=128):\n",
    "    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "train_encodings = encode_texts(train_texts)\n",
    "val_encodings = encode_texts(val_texts)\n",
    "test_encodings = encode_texts(test_texts)\n",
    "\n",
    "train_labels = torch.tensor(train_labels.values)\n",
    "val_labels = torch.tensor(val_labels.values)\n",
    "test_labels = torch.tensor(test_labels.values)\n",
    "\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c280778-4b52-4a2b-8466-f93699e3970c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "epochs = 5\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1 * total_steps, num_training_steps=total_steps)\n",
    "\n",
    "early_stopping_patience = 3\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8fd8bdd-dea9-4aa3-b7b1-7ba7f76e79ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.6453, Val Loss: 0.5661, Accuracy: 0.7527, Time: 28950.26s\n",
      "Epoch 2/5, Train Loss: 0.5155, Val Loss: 0.5487, Accuracy: 0.7654, Time: 28996.82s\n",
      "Epoch 3/5, Train Loss: 0.4140, Val Loss: 0.5952, Accuracy: 0.7581, Time: 28757.12s\n",
      "Epoch 4/5, Train Loss: 0.3196, Val Loss: 0.6786, Accuracy: 0.7521, Time: 29049.76s\n",
      "Epoch 5/5, Train Loss: 0.2500, Val Loss: 0.7781, Accuracy: 0.7504, Time: 29569.60s\n",
      "Early stopping triggered\n",
      "Total training time: 145323.55s\n",
      "Test Loss: 0.5424\n",
      "Test Accuracy: 0.7689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('bert_model_final/tokenizer_config.json',\n",
       " 'bert_model_final/special_tokens_map.json',\n",
       " 'bert_model_final/vocab.txt',\n",
       " 'bert_model_final/added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(model, train_dataloader, optimizer, scheduler, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    for batch in train_dataloader:\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "    end_time = time.time()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    epoch_time = end_time - start_time\n",
    "    return avg_train_loss, epoch_time\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "                loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == b_labels).sum().item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "total_training_time = 0\n",
    "for epoch in range(epochs):\n",
    "    train_loss, epoch_time = train_model(model, train_dataloader, optimizer, scheduler, scaler)\n",
    "    val_loss, accuracy = evaluate_model(model, val_dataloader)\n",
    "    total_training_time += epoch_time\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, Time: {epoch_time:.2f}s')\n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    model.save_pretrained(f'bert_model_epoch_{epoch+1}')\n",
    "    tokenizer.save_pretrained(f'bert_model_epoch_{epoch+1}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        model.save_pretrained('bert_model_best')\n",
    "        tokenizer.save_pretrained('bert_model_best')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "print(f'Total training time: {total_training_time:.2f}s')\n",
    "\n",
    "# Load the best model for final evaluation\n",
    "model = BertForSequenceClassification.from_pretrained('bert_model_best')\n",
    "test_loss, accuracy = evaluate_model(model, test_dataloader)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Save the final model\n",
    "model.save_pretrained('bert_model_final')\n",
    "tokenizer.save_pretrained('bert_model_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e89224-8f63-4f70-b3e6-b2c39c0aa24f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
